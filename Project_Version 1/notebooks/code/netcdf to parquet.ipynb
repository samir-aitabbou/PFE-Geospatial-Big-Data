{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548fb1de",
   "metadata": {},
   "source": [
    "Yes, you can use the xarray and pyarrow libraries in Python to convert netCDF files to Parquet format. Here's some example code to get you started:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02826161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping pyarrow as it is not installed.\n"
     ]
    }
   ],
   "source": [
    "pip uninstall pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5433e26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Using cached pyarrow-11.0.0-cp311-cp311-win_amd64.whl (20.5 MB)\n",
      "Requirement already satisfied: numpy>=1.16.6 in c:\\users\\samir\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyarrow) (1.24.1)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-11.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.0.1 -> 23.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2fe04ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyarrow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxr\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpq\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyarrow'"
     ]
    }
   ],
   "source": [
    "import xarray as xr\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c1ed0a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\xarray\\coding\\times.py:153: SerializationWarning: Ambiguous reference date string: 1-1-1 00:00:0.0. The first value is assumed to be the year hence will be padded with zeros to remove the ambiguity (the padded reference date string is: 0001-1-1 00:00:0.0). To remove this message, remove the ambiguity by padding your reference date strings with zeros.\n",
      "  warnings.warn(warning_msg, SerializationWarning)\n"
     ]
    }
   ],
   "source": [
    "# open the netCDF file with xarray\n",
    "ds = xr.open_dataset('air.sig995.2012.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a2ea186c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the NetCDF file is 7748542 bytes (7.39 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# # Specify the path to the NetCDF file\n",
    "file_path = 'air.sig995.2012.nc'\n",
    "\n",
    "# Use the os.path.getsize() function to get the size of the file in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "\n",
    "# Convert the file size from bytes to megabytes (optional)\n",
    "file_size_mb = file_size / (1024 * 1024)\n",
    "\n",
    "# Print the file size\n",
    "print(f\"The size of the NetCDF file is {file_size} bytes ({file_size_mb:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ada8433c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body[data-theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: 'â–º';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: 'â–¼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-index-preview {\n",
       "  grid-column: 2 / 5;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data,\n",
       ".xr-index-data-in:checked ~ .xr-index-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-index-name div,\n",
       ".xr-index-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data,\n",
       ".xr-index-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2,\n",
       ".xr-no-icon {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.Dataset&gt;\n",
       "Dimensions:  (lat: 73, lon: 144, time: 366)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
       "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
       "  * time     (time) datetime64[ns] 2012-01-01 2012-01-02 ... 2012-12-31\n",
       "Data variables:\n",
       "    air      (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:  COARDS\n",
       "    title:        mean daily NMC reanalysis (2012)\n",
       "    history:      created 2011/12 by Hoop (netCDF2.3)\n",
       "    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n",
       "    platform:     Model\n",
       "    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly...</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.Dataset</div></div><ul class='xr-sections'><li class='xr-section-item'><input id='section-ea34e09c-926c-4547-9fa3-19bbd788fe34' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-ea34e09c-926c-4547-9fa3-19bbd788fe34' class='xr-section-summary'  title='Expand/collapse section'>Dimensions:</label><div class='xr-section-inline-details'><ul class='xr-dim-list'><li><span class='xr-has-index'>lat</span>: 73</li><li><span class='xr-has-index'>lon</span>: 144</li><li><span class='xr-has-index'>time</span>: 366</li></ul></div><div class='xr-section-details'></div></li><li class='xr-section-item'><input id='section-7ff3e5ca-89dd-404c-9cbd-d32beaf32ca6' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7ff3e5ca-89dd-404c-9cbd-d32beaf32ca6' class='xr-section-summary' >Coordinates: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lat</span></div><div class='xr-var-dims'>(lat)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>90.0 87.5 85.0 ... -87.5 -90.0</div><input id='attrs-89ff37c1-333d-4d28-9b44-072055b2a7c5' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-89ff37c1-333d-4d28-9b44-072055b2a7c5' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-9bea4a06-5b57-405b-8093-72316eeb55ea' class='xr-var-data-in' type='checkbox'><label for='data-9bea4a06-5b57-405b-8093-72316eeb55ea' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_north</dd><dt><span>actual_range :</span></dt><dd>[ 90. -90.]</dd><dt><span>long_name :</span></dt><dd>Latitude</dd><dt><span>standard_name :</span></dt><dd>latitude</dd><dt><span>axis :</span></dt><dd>Y</dd></dl></div><div class='xr-var-data'><pre>array([ 90. ,  87.5,  85. ,  82.5,  80. ,  77.5,  75. ,  72.5,  70. ,  67.5,\n",
       "        65. ,  62.5,  60. ,  57.5,  55. ,  52.5,  50. ,  47.5,  45. ,  42.5,\n",
       "        40. ,  37.5,  35. ,  32.5,  30. ,  27.5,  25. ,  22.5,  20. ,  17.5,\n",
       "        15. ,  12.5,  10. ,   7.5,   5. ,   2.5,   0. ,  -2.5,  -5. ,  -7.5,\n",
       "       -10. , -12.5, -15. , -17.5, -20. , -22.5, -25. , -27.5, -30. , -32.5,\n",
       "       -35. , -37.5, -40. , -42.5, -45. , -47.5, -50. , -52.5, -55. , -57.5,\n",
       "       -60. , -62.5, -65. , -67.5, -70. , -72.5, -75. , -77.5, -80. , -82.5,\n",
       "       -85. , -87.5, -90. ], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>lon</span></div><div class='xr-var-dims'>(lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>0.0 2.5 5.0 ... 352.5 355.0 357.5</div><input id='attrs-993c7c77-f65b-416c-834c-ed7299fb5f3c' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-993c7c77-f65b-416c-834c-ed7299fb5f3c' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b17a7523-a136-4020-a817-7e168faf25a3' class='xr-var-data-in' type='checkbox'><label for='data-b17a7523-a136-4020-a817-7e168faf25a3' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>units :</span></dt><dd>degrees_east</dd><dt><span>long_name :</span></dt><dd>Longitude</dd><dt><span>actual_range :</span></dt><dd>[  0.  357.5]</dd><dt><span>standard_name :</span></dt><dd>longitude</dd><dt><span>axis :</span></dt><dd>X</dd></dl></div><div class='xr-var-data'><pre>array([  0. ,   2.5,   5. ,   7.5,  10. ,  12.5,  15. ,  17.5,  20. ,  22.5,\n",
       "        25. ,  27.5,  30. ,  32.5,  35. ,  37.5,  40. ,  42.5,  45. ,  47.5,\n",
       "        50. ,  52.5,  55. ,  57.5,  60. ,  62.5,  65. ,  67.5,  70. ,  72.5,\n",
       "        75. ,  77.5,  80. ,  82.5,  85. ,  87.5,  90. ,  92.5,  95. ,  97.5,\n",
       "       100. , 102.5, 105. , 107.5, 110. , 112.5, 115. , 117.5, 120. , 122.5,\n",
       "       125. , 127.5, 130. , 132.5, 135. , 137.5, 140. , 142.5, 145. , 147.5,\n",
       "       150. , 152.5, 155. , 157.5, 160. , 162.5, 165. , 167.5, 170. , 172.5,\n",
       "       175. , 177.5, 180. , 182.5, 185. , 187.5, 190. , 192.5, 195. , 197.5,\n",
       "       200. , 202.5, 205. , 207.5, 210. , 212.5, 215. , 217.5, 220. , 222.5,\n",
       "       225. , 227.5, 230. , 232.5, 235. , 237.5, 240. , 242.5, 245. , 247.5,\n",
       "       250. , 252.5, 255. , 257.5, 260. , 262.5, 265. , 267.5, 270. , 272.5,\n",
       "       275. , 277.5, 280. , 282.5, 285. , 287.5, 290. , 292.5, 295. , 297.5,\n",
       "       300. , 302.5, 305. , 307.5, 310. , 312.5, 315. , 317.5, 320. , 322.5,\n",
       "       325. , 327.5, 330. , 332.5, 335. , 337.5, 340. , 342.5, 345. , 347.5,\n",
       "       350. , 352.5, 355. , 357.5], dtype=float32)</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>time</span></div><div class='xr-var-dims'>(time)</div><div class='xr-var-dtype'>datetime64[ns]</div><div class='xr-var-preview xr-preview'>2012-01-01 ... 2012-12-31</div><input id='attrs-8e52cf22-31c7-425b-bed5-e5c566d4b7cf' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-8e52cf22-31c7-425b-bed5-e5c566d4b7cf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-04b661e0-55a4-40dd-85df-f5f492aa1bf6' class='xr-var-data-in' type='checkbox'><label for='data-04b661e0-55a4-40dd-85df-f5f492aa1bf6' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>Time</dd><dt><span>actual_range :</span></dt><dd>[17628096. 17636856.]</dd><dt><span>delta_t :</span></dt><dd>0000-00-01 00:00:00</dd><dt><span>standard_name :</span></dt><dd>time</dd><dt><span>axis :</span></dt><dd>T</dd><dt><span>avg_period :</span></dt><dd>0000-00-01 00:00:00</dd></dl></div><div class='xr-var-data'><pre>array([&#x27;2012-01-01T00:00:00.000000000&#x27;, &#x27;2012-01-02T00:00:00.000000000&#x27;,\n",
       "       &#x27;2012-01-03T00:00:00.000000000&#x27;, ..., &#x27;2012-12-29T00:00:00.000000000&#x27;,\n",
       "       &#x27;2012-12-30T00:00:00.000000000&#x27;, &#x27;2012-12-31T00:00:00.000000000&#x27;],\n",
       "      dtype=&#x27;datetime64[ns]&#x27;)</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-aed513f6-908c-4cc9-babe-64e5d11191d3' class='xr-section-summary-in' type='checkbox'  checked><label for='section-aed513f6-908c-4cc9-babe-64e5d11191d3' class='xr-section-summary' >Data variables: <span>(1)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span>air</span></div><div class='xr-var-dims'>(time, lat, lon)</div><div class='xr-var-dtype'>float32</div><div class='xr-var-preview xr-preview'>...</div><input id='attrs-2b5f5bda-cf74-40d0-a215-2ad4fe7a1c1e' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-2b5f5bda-cf74-40d0-a215-2ad4fe7a1c1e' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d725a4ed-e06d-463d-848e-53e4e4bd6365' class='xr-var-data-in' type='checkbox'><label for='data-d725a4ed-e06d-463d-848e-53e4e4bd6365' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>long_name :</span></dt><dd>mean Daily Air temperature at sigma level 995</dd><dt><span>unpacked_valid_range :</span></dt><dd>[185.16 331.16]</dd><dt><span>actual_range :</span></dt><dd>[193.80002 317.52002]</dd><dt><span>units :</span></dt><dd>degK</dd><dt><span>precision :</span></dt><dd>2</dd><dt><span>GRIB_id :</span></dt><dd>11</dd><dt><span>GRIB_name :</span></dt><dd>TMP</dd><dt><span>var_desc :</span></dt><dd>Air temperature</dd><dt><span>dataset :</span></dt><dd>NCEP Reanalysis Daily Averages</dd><dt><span>level_desc :</span></dt><dd>Surface</dd><dt><span>statistic :</span></dt><dd>Mean\n",
       "M</dd><dt><span>parent_stat :</span></dt><dd>Individual Obs\n",
       "I</dd><dt><span>valid_range :</span></dt><dd>[-32765 -18165]</dd></dl></div><div class='xr-var-data'><pre>[3847392 values with dtype=float32]</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-0236a322-19c1-490c-bfea-35cc47a9db58' class='xr-section-summary-in' type='checkbox'  ><label for='section-0236a322-19c1-490c-bfea-35cc47a9db58' class='xr-section-summary' >Indexes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-index-name'><div>lat</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-9af0c27d-2945-4c26-836f-0ae8d26513b7' class='xr-index-data-in' type='checkbox'/><label for='index-9af0c27d-2945-4c26-836f-0ae8d26513b7' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([ 90.0,  87.5,  85.0,  82.5,  80.0,  77.5,  75.0,  72.5,  70.0,\n",
       "               67.5,  65.0,  62.5,  60.0,  57.5,  55.0,  52.5,  50.0,  47.5,\n",
       "               45.0,  42.5,  40.0,  37.5,  35.0,  32.5,  30.0,  27.5,  25.0,\n",
       "               22.5,  20.0,  17.5,  15.0,  12.5,  10.0,   7.5,   5.0,   2.5,\n",
       "                0.0,  -2.5,  -5.0,  -7.5, -10.0, -12.5, -15.0, -17.5, -20.0,\n",
       "              -22.5, -25.0, -27.5, -30.0, -32.5, -35.0, -37.5, -40.0, -42.5,\n",
       "              -45.0, -47.5, -50.0, -52.5, -55.0, -57.5, -60.0, -62.5, -65.0,\n",
       "              -67.5, -70.0, -72.5, -75.0, -77.5, -80.0, -82.5, -85.0, -87.5,\n",
       "              -90.0],\n",
       "             dtype=&#x27;float64&#x27;, name=&#x27;lat&#x27;))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>lon</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-2149f1ba-6948-482a-9afa-f4dfc7e69fe6' class='xr-index-data-in' type='checkbox'/><label for='index-2149f1ba-6948-482a-9afa-f4dfc7e69fe6' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(Float64Index([  0.0,   2.5,   5.0,   7.5,  10.0,  12.5,  15.0,  17.5,  20.0,\n",
       "               22.5,\n",
       "              ...\n",
       "              335.0, 337.5, 340.0, 342.5, 345.0, 347.5, 350.0, 352.5, 355.0,\n",
       "              357.5],\n",
       "             dtype=&#x27;float64&#x27;, name=&#x27;lon&#x27;, length=144))</pre></div></li><li class='xr-var-item'><div class='xr-index-name'><div>time</div></div><div class='xr-index-preview'>PandasIndex</div><div></div><input id='index-08313030-7eae-4956-8d45-5ab81473aec3' class='xr-index-data-in' type='checkbox'/><label for='index-08313030-7eae-4956-8d45-5ab81473aec3' title='Show/Hide index repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-index-data'><pre>PandasIndex(DatetimeIndex([&#x27;2012-01-01&#x27;, &#x27;2012-01-02&#x27;, &#x27;2012-01-03&#x27;, &#x27;2012-01-04&#x27;,\n",
       "               &#x27;2012-01-05&#x27;, &#x27;2012-01-06&#x27;, &#x27;2012-01-07&#x27;, &#x27;2012-01-08&#x27;,\n",
       "               &#x27;2012-01-09&#x27;, &#x27;2012-01-10&#x27;,\n",
       "               ...\n",
       "               &#x27;2012-12-22&#x27;, &#x27;2012-12-23&#x27;, &#x27;2012-12-24&#x27;, &#x27;2012-12-25&#x27;,\n",
       "               &#x27;2012-12-26&#x27;, &#x27;2012-12-27&#x27;, &#x27;2012-12-28&#x27;, &#x27;2012-12-29&#x27;,\n",
       "               &#x27;2012-12-30&#x27;, &#x27;2012-12-31&#x27;],\n",
       "              dtype=&#x27;datetime64[ns]&#x27;, name=&#x27;time&#x27;, length=366, freq=None))</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-173ef1aa-3adc-4139-a673-2443fd704069' class='xr-section-summary-in' type='checkbox'  checked><label for='section-173ef1aa-3adc-4139-a673-2443fd704069' class='xr-section-summary' >Attributes: <span>(6)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>Conventions :</span></dt><dd>COARDS</dd><dt><span>title :</span></dt><dd>mean daily NMC reanalysis (2012)</dd><dt><span>history :</span></dt><dd>created 2011/12 by Hoop (netCDF2.3)</dd><dt><span>description :</span></dt><dd>Data is from NMC initialized reanalysis\n",
       "(4x/day).  These are the 0.9950 sigma level values.</dd><dt><span>platform :</span></dt><dd>Model</dd><dt><span>references :</span></dt><dd>http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanalysis.html</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.Dataset>\n",
       "Dimensions:  (lat: 73, lon: 144, time: 366)\n",
       "Coordinates:\n",
       "  * lat      (lat) float32 90.0 87.5 85.0 82.5 80.0 ... -82.5 -85.0 -87.5 -90.0\n",
       "  * lon      (lon) float32 0.0 2.5 5.0 7.5 10.0 ... 350.0 352.5 355.0 357.5\n",
       "  * time     (time) datetime64[ns] 2012-01-01 2012-01-02 ... 2012-12-31\n",
       "Data variables:\n",
       "    air      (time, lat, lon) float32 ...\n",
       "Attributes:\n",
       "    Conventions:  COARDS\n",
       "    title:        mean daily NMC reanalysis (2012)\n",
       "    history:      created 2011/12 by Hoop (netCDF2.3)\n",
       "    description:  Data is from NMC initialized reanalysis\\n(4x/day).  These a...\n",
       "    platform:     Model\n",
       "    references:   http://www.esrl.noaa.gov/psd/data/gridded/data.ncep.reanaly..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f98ee2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the xarray dataset to a pandas dataframe\n",
    "df = ds.to_dataframe().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4bf14b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>time</th>\n",
       "      <th>air</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-01</td>\n",
       "      <td>234.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-02</td>\n",
       "      <td>235.339996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-03</td>\n",
       "      <td>238.700012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>240.130005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>90.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>256.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847387</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>357.5</td>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>252.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847388</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>357.5</td>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>250.420013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847389</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>357.5</td>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>250.470001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847390</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>357.5</td>\n",
       "      <td>2012-12-30</td>\n",
       "      <td>249.550018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3847391</th>\n",
       "      <td>-90.0</td>\n",
       "      <td>357.5</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>251.170013</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3847392 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          lat    lon       time         air\n",
       "0        90.0    0.0 2012-01-01  234.500000\n",
       "1        90.0    0.0 2012-01-02  235.339996\n",
       "2        90.0    0.0 2012-01-03  238.700012\n",
       "3        90.0    0.0 2012-01-04  240.130005\n",
       "4        90.0    0.0 2012-01-05  256.470001\n",
       "...       ...    ...        ...         ...\n",
       "3847387 -90.0  357.5 2012-12-27  252.470001\n",
       "3847388 -90.0  357.5 2012-12-28  250.420013\n",
       "3847389 -90.0  357.5 2012-12-29  250.470001\n",
       "3847390 -90.0  357.5 2012-12-30  249.550018\n",
       "3847391 -90.0  357.5 2012-12-31  251.170013\n",
       "\n",
       "[3847392 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64a9fd4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.0.0\n"
     ]
    }
   ],
   "source": [
    "import pyarrow\n",
    "print(pyarrow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83a02331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the pandas dataframe to a pyarrow table\n",
    "import pyarrow as pa\n",
    "table = pa.Table.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a499f80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the pyarrow table to a Parquet file\n",
    "pa.parquet.write_table(table, 'parquet_file_1.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4301019d",
   "metadata": {},
   "source": [
    "You can then read the Parquet file into Spark using the spark.read.parquet function. For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5feea43b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the parquet file is 6547568 bytes (6.24 MB)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# # Specify the path to the NetCDF file\n",
    "file_path = 'parquet_file_1.parquet'\n",
    "\n",
    "# Use the os.path.getsize() function to get the size of the file in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "\n",
    "# Convert the file size from bytes to megabytes (optional)\n",
    "file_size_mb = file_size / (1024 * 1024)\n",
    "\n",
    "# Print the file size\n",
    "print(f\"The size of the parquet file is {file_size} bytes ({file_size_mb:.2f} MB)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b6757d",
   "metadata": {},
   "source": [
    "# initiate Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca11c68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('netcdf-to-parquet').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cf379fb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Read the Parquet file into a DataFrame\n",
    "df1 = spark.read.parquet('parquet_file_1.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "febfa024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| lat|      air|\n",
      "+----+---------+\n",
      "|90.0|    234.5|\n",
      "|90.0|   235.34|\n",
      "|90.0|238.70001|\n",
      "|90.0|   240.13|\n",
      "|90.0|   256.47|\n",
      "|90.0|260.65002|\n",
      "|90.0|    256.1|\n",
      "|90.0|   244.78|\n",
      "|90.0|   253.43|\n",
      "|90.0|   244.38|\n",
      "|90.0|245.73001|\n",
      "|90.0|   241.18|\n",
      "|90.0|   244.93|\n",
      "|90.0|248.23001|\n",
      "|90.0|   253.53|\n",
      "|90.0|268.65002|\n",
      "|90.0|    259.1|\n",
      "|90.0|   260.43|\n",
      "|90.0|   256.68|\n",
      "|90.0|251.55002|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select a subset of variables\n",
    "subset = df1.select(col('lat'), col('air'))\n",
    "subset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6c199e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------------+---------+\n",
      "| lat|lon|               time|      air|\n",
      "+----+---+-------------------+---------+\n",
      "|90.0|0.0|2012-01-01 00:00:00|    234.5|\n",
      "|90.0|0.0|2012-01-02 00:00:00|   235.34|\n",
      "|90.0|0.0|2012-01-03 00:00:00|238.70001|\n",
      "|90.0|0.0|2012-01-04 00:00:00|   240.13|\n",
      "|90.0|0.0|2012-01-05 00:00:00|   256.47|\n",
      "|90.0|0.0|2012-01-06 00:00:00|260.65002|\n",
      "|90.0|0.0|2012-01-07 00:00:00|    256.1|\n",
      "|90.0|0.0|2012-01-08 00:00:00|   244.78|\n",
      "|90.0|0.0|2012-01-09 00:00:00|   253.43|\n",
      "|90.0|0.0|2012-01-10 00:00:00|   244.38|\n",
      "|90.0|0.0|2012-01-11 00:00:00|245.73001|\n",
      "|90.0|0.0|2012-01-12 00:00:00|   241.18|\n",
      "|90.0|0.0|2012-01-13 00:00:00|   244.93|\n",
      "|90.0|0.0|2012-01-14 00:00:00|248.23001|\n",
      "|90.0|0.0|2012-01-15 00:00:00|   253.53|\n",
      "|90.0|0.0|2012-01-16 00:00:00|268.65002|\n",
      "|90.0|0.0|2012-01-17 00:00:00|    259.1|\n",
      "|90.0|0.0|2012-01-18 00:00:00|   260.43|\n",
      "|90.0|0.0|2012-01-19 00:00:00|   256.68|\n",
      "|90.0|0.0|2012-01-20 00:00:00|251.55002|\n",
      "+----+---+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the data based on a condition\n",
    "filtered = df1.filter(col('lat') > 80)\n",
    "filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "58fa73d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+\n",
      "|               time|          avg(air)|\n",
      "+-------------------+------------------+\n",
      "|2012-01-22 00:00:00|276.23148196726993|\n",
      "|2012-07-11 00:00:00|280.33243449020966|\n",
      "|2012-10-20 00:00:00|278.54230460656106|\n",
      "|2012-01-16 00:00:00|277.76814804483587|\n",
      "|2012-12-17 00:00:00| 276.6795621230359|\n",
      "|2012-01-19 00:00:00|277.41109470036474|\n",
      "|2012-05-12 00:00:00|279.18268508272445|\n",
      "|2012-10-10 00:00:00| 278.5887950159825|\n",
      "|2012-12-18 00:00:00|276.55451096719077|\n",
      "|2012-04-23 00:00:00| 277.5920088984288|\n",
      "|2012-07-08 00:00:00| 280.2102692573582|\n",
      "|2012-08-24 00:00:00| 280.4929269909677|\n",
      "|2012-03-22 00:00:00| 276.0524667191179|\n",
      "|2012-04-20 00:00:00| 277.2611756898134|\n",
      "|2012-07-23 00:00:00| 280.7047726298758|\n",
      "|2012-03-30 00:00:00| 275.6068892195889|\n",
      "|2012-05-05 00:00:00| 278.6593580202425|\n",
      "|2012-01-26 00:00:00| 275.9756685776616|\n",
      "|2012-05-07 00:00:00|279.06678966481576|\n",
      "|2012-07-22 00:00:00|280.75354669591246|\n",
      "+-------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Group the data by a dimension and compute the average of a variable\n",
    "grouped = df1.groupBy(col('time')).agg({'air': 'avg'})\n",
    "grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "4f42d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[lat: double, lon: double, Time: timestamp_ntz, air: float]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rename variables\n",
    "renamed = df1.withColumnRenamed('time', 'Time')\n",
    "renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1dd2d16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new variable\n",
    "# new_col = df1.withColumn('3*air', df.air * 3)\n",
    "# new_col.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "06d65df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data using pivot\n",
    "# pivoted = df.groupBy(col('time')).pivot('lat').agg({'air': 'avg'})\n",
    "# pivoted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "25fde0f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------------------+---------+\n",
      "|  lat|  lon|               time|      air|\n",
      "+-----+-----+-------------------+---------+\n",
      "|-75.0|107.5|2012-06-02 00:00:00|193.80002|\n",
      "|-75.0|105.0|2012-06-02 00:00:00|   194.13|\n",
      "|-75.0|117.5|2012-06-03 00:00:00|    194.6|\n",
      "|-80.0|125.0|2012-06-09 00:00:00|   194.68|\n",
      "|-75.0|110.0|2012-06-02 00:00:00|194.70001|\n",
      "|-80.0|122.5|2012-06-09 00:00:00|194.70001|\n",
      "|-75.0|115.0|2012-06-03 00:00:00|   194.75|\n",
      "|-77.5|105.0|2012-06-09 00:00:00|    194.9|\n",
      "|-75.0|120.0|2012-06-03 00:00:00|    195.0|\n",
      "|-77.5|102.5|2012-06-09 00:00:00|    195.0|\n",
      "|-80.0|127.5|2012-06-09 00:00:00|    195.0|\n",
      "|-75.0|115.0|2012-08-19 00:00:00|195.05002|\n",
      "|-80.0|120.0|2012-06-09 00:00:00|    195.1|\n",
      "|-75.0|117.5|2012-08-19 00:00:00|   195.13|\n",
      "|-77.5|107.5|2012-06-09 00:00:00|   195.13|\n",
      "|-77.5|120.0|2012-06-08 00:00:00|   195.13|\n",
      "|-75.0|112.5|2012-08-19 00:00:00|195.30002|\n",
      "|-75.0|112.5|2012-06-03 00:00:00|   195.34|\n",
      "|-75.0|122.5|2012-06-08 00:00:00|   195.34|\n",
      "|-77.5|117.5|2012-06-08 00:00:00|   195.43|\n",
      "+-----+-----+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the data by a variable\n",
    "sorted = df1.orderBy(col('air'))\n",
    "sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb6455c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join two dataframes\n",
    "# other_df = ...\n",
    "# joined = df.join(other_df, on=col('latitude'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ca90db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a calculation on the data\n",
    "# calculation = df1.withColumn('ne_air', df.air * 2 - df.air/2)\n",
    "# calculation.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cc70460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+--------+\n",
      "|               time|          avg(air)|max(lat)|\n",
      "+-------------------+------------------+--------+\n",
      "|2012-01-22 00:00:00|276.23148196726993|    90.0|\n",
      "|2012-07-11 00:00:00|280.33243449020966|    90.0|\n",
      "|2012-10-20 00:00:00|278.54230460656106|    90.0|\n",
      "|2012-01-16 00:00:00|277.76814804483587|    90.0|\n",
      "|2012-12-17 00:00:00| 276.6795621230359|    90.0|\n",
      "|2012-01-19 00:00:00|277.41109470036474|    90.0|\n",
      "|2012-05-12 00:00:00|279.18268508272445|    90.0|\n",
      "|2012-10-10 00:00:00| 278.5887950159825|    90.0|\n",
      "|2012-12-18 00:00:00|276.55451096719077|    90.0|\n",
      "|2012-04-23 00:00:00| 277.5920088984288|    90.0|\n",
      "|2012-07-08 00:00:00| 280.2102692573582|    90.0|\n",
      "|2012-08-24 00:00:00| 280.4929269909677|    90.0|\n",
      "|2012-03-22 00:00:00| 276.0524667191179|    90.0|\n",
      "|2012-04-20 00:00:00| 277.2611756898134|    90.0|\n",
      "|2012-07-23 00:00:00| 280.7047726298758|    90.0|\n",
      "|2012-03-30 00:00:00| 275.6068892195889|    90.0|\n",
      "|2012-05-05 00:00:00| 278.6593580202425|    90.0|\n",
      "|2012-01-26 00:00:00| 275.9756685776616|    90.0|\n",
      "|2012-05-07 00:00:00|279.06678966481576|    90.0|\n",
      "|2012-07-22 00:00:00|280.75354669591246|    90.0|\n",
      "+-------------------+------------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aggregate the data by a dimension\n",
    "aggregated = df1.groupBy(col('time')).agg({'lat': 'max','air': 'avg'})\n",
    "aggregated.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd48a5",
   "metadata": {},
   "source": [
    "These examples demonstrate various data manipulation operations using Spark's DataFrame API, such as selecting subsets of variables, filtering the data, grouping by dimensions and computing aggregates, renaming variables, adding new variables, pivoting the data, sorting by variables, joining dataframes, performing calculations, and aggregating by dimensions.\n",
    "\n",
    "You can customize these operations to suit your specific needs, and combine them to perform complex data transformations and analysis. The DataFrame API offers a rich set of operations that enable you to work with Parquet files and other types of data efficiently and flexibly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82b93af",
   "metadata": {},
   "source": [
    "# filter a Spark dataframe based on time and save the results to a Parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a412ca64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+-------------------+---------+\n",
      "| lat|lon|               time|      air|\n",
      "+----+---+-------------------+---------+\n",
      "|90.0|0.0|2012-02-17 00:00:00|   259.45|\n",
      "|90.0|0.0|2012-02-20 00:00:00|    255.4|\n",
      "|90.0|0.0|2012-02-24 00:00:00|262.77002|\n",
      "|90.0|0.0|2012-02-25 00:00:00|257.65002|\n",
      "|90.0|0.0|2012-03-09 00:00:00|    263.3|\n",
      "|90.0|0.0|2012-03-10 00:00:00|   262.45|\n",
      "|90.0|0.0|2012-03-11 00:00:00|255.73001|\n",
      "|90.0|0.0|2012-04-03 00:00:00|    260.0|\n",
      "|90.0|0.0|2012-04-04 00:00:00|   262.22|\n",
      "|90.0|0.0|2012-04-05 00:00:00|   255.68|\n",
      "|90.0|0.0|2012-04-07 00:00:00|   256.28|\n",
      "|90.0|0.0|2012-04-13 00:00:00|    255.5|\n",
      "|90.0|0.0|2012-04-14 00:00:00|   259.55|\n",
      "|90.0|0.0|2012-04-15 00:00:00|   261.47|\n",
      "|90.0|0.0|2012-04-16 00:00:00|    258.3|\n",
      "|90.0|0.0|2012-04-17 00:00:00|   259.87|\n",
      "|90.0|0.0|2012-04-18 00:00:00|257.52002|\n",
      "|90.0|0.0|2012-04-19 00:00:00|258.15002|\n",
      "|90.0|0.0|2012-04-20 00:00:00|   256.12|\n",
      "|90.0|0.0|2012-04-28 00:00:00|257.40002|\n",
      "+----+---+-------------------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter the dataframe based on time, latitude, and air variable\n",
    "filtered_df = df1.filter(\"time > '2012-01-22' AND time < '2012-05-22' AND lat > 30 AND lat < 100 AND air > 255\")\n",
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1aa8c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = filtered_df.to_xarray()\n",
    "# ds.to_netcdf('filtered_data.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c6183bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the filtered dataframe to a new Parquet file\n",
    "# filtered_df.write.mode('overwrite').parquet(\"filtered_data.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "777cc4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_data = spark.read.parquet('filtered_data.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1871f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write spark dataframe into netcdf\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85516592",
   "metadata": {},
   "source": [
    "# install hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "2b2cf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import urllib.request\n",
    "# import zipfile\n",
    "\n",
    "# # Download and extract Hadoop\n",
    "# url = \"https://downloads.apache.org/hadoop/common/hadoop-3.3.1/hadoop-3.3.1.zip\"\n",
    "# filename = \"hadoop.zip\"\n",
    "# urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "# with zipfile.ZipFile(filename, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall(\"C:/hadoop\")\n",
    "\n",
    "# # Set environment variables\n",
    "# os.environ[\"HADOOP_HOME\"] = \"C:/hadoop/hadoop-3.3.1\"\n",
    "# # os.environ[\"JAVA_HOME\"] = \"C:/Program Files/Java/jdk1.8.0_281\"\n",
    "\n",
    "# # Copy configuration files\n",
    "# src = os.path.join(os.getcwd(), \"config\")\n",
    "# dst = os.path.join(os.environ[\"HADOOP_HOME\"], \"etc\", \"hadoop\")\n",
    "# shutil.copytree(src, dst)\n",
    "\n",
    "# # Start Hadoop services\n",
    "# os.system(os.path.join(os.environ[\"HADOOP_HOME\"], \"sbin\", \"start-dfs.cmd\"))\n",
    "# os.system(os.path.join(os.environ[\"HADOOP_HOME\"], \"sbin\", \"start-yarn.cmd\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "72af9ea5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o166.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[82], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Write the modified DataFrame back to Parquet format\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mfiltered_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msss.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:1656\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1655\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1656\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o166.parquet.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:735)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:270)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:286)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:978)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:660)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:700)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:699)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:672)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:788)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:188)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:269)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:789)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:578)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1623)\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:547)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:568)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:591)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:688)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1907)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1867)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1840)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:181)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:50)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:48)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<init>(ShutdownHookManager.scala:58)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:341)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:331)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:370)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:955)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:192)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:215)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:91)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1111)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1120)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:467)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:438)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:515)\r\n\t... 23 more\r\n"
     ]
    }
   ],
   "source": [
    "# Write the modified DataFrame back to Parquet format\n",
    "\n",
    "filtered_df.write.parquet('filtered_df.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d7eea574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Write the modified DataFrame back to Parquet format\n",
    "# # filtered_df.write.format('parquet').save('samir_2_1.parquet')\n",
    "\n",
    "# import pyarrow as pa\n",
    "# import pyarrow.parquet as pq\n",
    "\n",
    "# # Convert the Spark DataFrame to a Pandas DataFrame\n",
    "# df = filtered_df.toPandas()\n",
    "\n",
    "# # Create a xarray Dataset from the Pandas DataFrame\n",
    "# ds = xr.Dataset.from_dataframe(df)\n",
    "# # Convert the xarray dataset to a PyArrow Table\n",
    "# table = pa.Table.from_pydict(ds.to_dict())\n",
    "\n",
    "# # Write the PyArrow Table to a Parquet file\n",
    "# pq.write_table(table, 'output.parquet')\n",
    "\n",
    "# # filtered_df.write.parquet('output.parquet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918b018",
   "metadata": {},
   "source": [
    "# convert parquet file back to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ad47bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samir\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    }
   ],
   "source": [
    "# Write the filtered dataframe to a NetCDF file using xarray\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "df = filtered_df.toPandas()\n",
    "\n",
    "# Create a xarray Dataset from the Pandas DataFrame\n",
    "ds = xr.Dataset.from_dataframe(df)\n",
    "\n",
    "# Write the xarray Dataset to a netCDF file\n",
    "ds.to_netcdf('netcdf_file.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1b6b7605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load NetCDF file using xarray\n",
    "# data = xr.open_dataset('netcdf_file.nc')\n",
    "\n",
    "# var_ = data.variables['time'][:]\n",
    "# var_.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a86f0",
   "metadata": {},
   "source": [
    "Note that this code assumes that your netCDF file contains a single dataset, and that the dataset can fit into memory as a pandas dataframe. If your file is large or contains multiple datasets, you may need to modify the code accordingly.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d605b4cd",
   "metadata": {},
   "source": [
    "If you have a large netCDF file that cannot fit into memory as a pandas dataframe, you can use dask to read and process the data in parallel, and then convert it to a PyArrow table and write it to Parquet format.\n",
    "\n",
    "Here's some example code that uses dask to read a netCDF file, chunk it into smaller pieces, and write each chunk to a separate Parquet file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c69547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask.bag as db\n",
    "import dask.dataframe as dd\n",
    "import xarray as xr\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# open the netCDF file with xarray\n",
    "ds = xr.open_dataset('your_netcdf_file.nc', chunks={'time': 100})\n",
    "\n",
    "# convert the xarray dataset to a dask array\n",
    "data = da.from_array(ds.to_array().data)\n",
    "\n",
    "# define a function to convert a dask array chunk to a PyArrow table and write it to Parquet\n",
    "def process_chunk(chunk):\n",
    "    df = dd.from_dask_array(chunk)\n",
    "    table = pa.Table.from_pandas(df.compute())\n",
    "    pq.write_table(table, 'chunk.parquet')\n",
    "\n",
    "# process each chunk in parallel using dask.bag\n",
    "chunks = db.from_array(data.blocks)\n",
    "chunks.map(process_chunk).compute()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d871a31",
   "metadata": {},
   "source": [
    "This code reads the netCDF file in chunks of 100 time steps, converts each chunk to a dask array, and then processes each chunk in parallel using dask.bag. The process_chunk function converts each chunk to a dask dataframe, computes it, converts it to a PyArrow table, and writes it to a separate Parquet file. The compute method is called to trigger the computation of the dask dataframe, which will be distributed across the available cores.\n",
    "\n",
    "Note that this code assumes that the variable of interest is stored as a single variable in the netCDF file, and that the variable has a time dimension. You may need to modify the code if your file is structured differently. Also, you may need to experiment with the chunk size to find a suitable value for your specific file and system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841734a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
